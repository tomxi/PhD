\section{Introduction}
Musical structure is a central matter in the human musical experience, and has received attention from many fields, including Music Theory, Music Cognition, and Music Informatics.
Many different methods and styles exist to analyze and communicate the structure of musical works, each analyzing different representations of the musical work, and aiming towards different goals.
For example, a Schenkerian analysis of a musical work prioritizes communicating the tonal organization of the work for performers or listeners; while an online music streaming service provider might want to pick out the most appropriate 15 seconds of any track in their catalogue for consumer previewing.
These different priorities contribute to the subtleties and complexities associated with analyzing musical structures.

While musical structure can be communicated via many different formats, a common approach for performing Music Structure Analysis (MSA) in the Music Information Retrieval (MIR) community is to segment the musical work into non-overlapping labeled segments.
These segments can be the verses, bridges, choruses of a pop song; they can also be different chords in a chord progression.
% Unlike other MSA related tasks such as pattern discovery, labeled segmentation is a task formulation where human annotation would be efficiently generated.
With the emergence of several public datasets of music segmentation annotations in the past decade, data-driven methods have consistently replaced classical methods to become state of the art in accuracy.
However, the development of data-driven MSA methods is stymied by data biases and conflicting annotations created by annotators who hold different, but often equally valid, interpretations that reflect their own priorities.

While other MSA tasks like pattern discovery and other representations of musical structure exist, this work will focus on the segmentation task, because I believe successful auditory segmentation is a necessary precursor for music appreciation and communication. 
That being said, the status quo of merely identifying the audio segmentation boundaries leave much to be desired of a structure analysis.
While musical structures can be intricate and deeply nuanced, a labeled segmentation presents only a highly compressed view, and more explanations as to why a model decided to transition to a new segment, as to merely when, is desired.
% Explainability
% In this context, a primary goal of this work is to provide more explainability to data-driven MSA models.
More interpretable segmentation models could be achieved, for example, by showing how individual musical features influence the model's overall segmentation, or qualify the decision for any of the section transitions by providing similar instances in training data as evidence for each.
Explainable segmentation produced by more interpretable MSA models could help explain inter-annotator disagreement, and help with potential adoption of the model for more user specific use case scenarios.

% Output format, system design, and techniques used for MSA are all heavily influenced by the context and use cases under which the analysis would be useful. 
% Different tasks like cover song detection, music thumb-nailing, or automatic music generation will directly impact the selection of features, methods, and models for MSA. 
% Across input modalities and downstream tasks, an emerging theme is to produce an explainable segmentation of the musical work that encapsulates much relevant knowledge.

\subsection{Need for Study}
While automatic MSA has seen robust development in the past two decades, automatic MSA tools still have not yet seen heavy user adoption outside of academia, and their outputs lack explainability in general. 
Currently, a typical automatic MSA pipeline generally involves 3 steps: 1) select and compute relevant musical features for each granular time point of the audio or musical score under analysis; 2) compare the features obtained for each time point with every other time point, and collect the result in the Self-Similarity Matrix (SSM); and 3) perform post-processing of the SSM to generate a labeled segmentation of the musical work. 
Current methods cannot resolve ambiguity or disagreement in annotation, and they are unable to meaningfully incorporate multiple interpretations or perspectives. 
Furthermore, the lack of explainability and specificity of MSA outputs are inhibiting their application and utility from impacting more real-world scenarios and use cases.

\subsection{Intended Contributions}

My hope is to incorporate two new elements into the typical MSA pipeline to address the lack of explainability of current automatic MSA methods.
% \cite{molnar2019} points out several ways to achieve interpretable data-driven methods.
% Amongst them, two major strategies are to focus on the relationship between individual features and predictions, and the relationship between training data points and predictions respectively.
% I would like to improve on the explainability of MSA segmentation outputs by incorporating two ideas:

First, I would like to develop methods to understand how each individual feature and the structure thereof influences the final segmentation, and work towards recognizing concurrently unfolding structures. 
Traditional MSA only produces a single segmentation that represents the structure of the audio excerpt in question. 
It is well known that the segmentation is heavily influenced by the choice of musical features, and different structures surface accordingly. 
In the case for segmentation boundaries, an opportunity for explainability might be to show how much each individual feature contributes to a certain decision, or perhaps how the structures of the individual feature themselves influence the overall predicted structure.
While traditionally, a choice is made to present a single structure (potentially out of a family of hierarchically consistent structures), I would like to promote the view that counterpoints between multiple concurrent structures (that are not necessarily hierarchically consistent) are equally, if not more, commonplace. 
Take for example a rhythmic guitar strumming passage over a jagged harmonic progression, interesting structures can be learnt by analyzing either just the rhythmic aspect, or just the chord progression. 
The counterpoint between the rhythmic and harmonic structures should not be obfuscated by merging the two.

Secondly, inspired by Neo-Riemannian theories, a branch of Transformational theory, I wish to shift the focus towards the transition between musical segments, as opposed to their individual identities.
% First, given a predicted segmentation, the MSA model should be able to provide additional explanation for each predicted boundary in terms of similar data points, or prototypes, the model has seen during training that can support its decision under question.
% This requires finding good representations for the segment boundaries, and elevate their importance to the same level as the segments, which are typically labeled.
% Currently, in a labeled segmentation, typically only the segments are labeled, but not the segment boundaries.
% This shift of focus towards the transitional boundaries between musical segments, as opposed to the individual identities of the segments themselves, is also in part inspired by Neo-Riemannian theories, a branch of Transformational theory.
In Neo-Riemannian theories, the segments are typically harmonies, and the representation for different types of segment transitions are often facilitated by topologies induced by the relationship between consecutive segments.
A favorite such topology in Neo-Riemannian theory is the Tonnetz, where different relationships between triads are represented by pairs of triangular faces in different orientations. 
More generally, by connecting related musical segments based on their relationships, a graph much like the Tonnetz can be formed to inform the relationship between segments. 
% Such graphs represent underlying topologies that detail the relationships between segments. 
By recognizing and identifying different types of segment transitions based on inter-segmental relationship, models would be at a better position to provide supporting examples from their training data that can justify their decisions.
% Seems like an awkward edit, don't know what's going on ^^
It is also remarkable how this idea of relationship between elements is closely related to the concept of kernel learning in mathematics.

% To summarize, I wish to improve the explainability and specificity of automatic MSA on two separate fronts: 1) to make segment boundaries more specific by incorporating topological relationships that’s inherent in musical elements, and 2) to analyze features independently and recognize concurrently unfolding structures and how this ensemble of structures can allow for possibility of multiple valid and distinct interpretations or annotations of the same piece of music.

% \subsection{Limitations and Delimitations}
% The availability of annotated music data limits this study by dictating the genre of audio recording and types of structure that gets analyzed, and perhaps imitated by a data-driven MSA model. 
% Furthermore, these data also dictate the objective evaluation that is typical in assessing a data-driven algorithm.

% I’d like to delimitate my research by focusing on aspects of automatic MSA that aim to solve specific problems, for example to facilitate music generation. 
% This research will also not focus deeply on the cognitive aspects of how human perceive musical structures.

\subsection{Research Question}
\begin{itemize}
	\item How can incorporating topological relationships and analyzing different musical features independently help improve the explainability of automatic music structure analysis?
\end{itemize}

\subsubsection{Sub Questions}
\begin{itemize}
	\item Can analysis of individual musical features separately as well as together help explain predicted segmentation boundaries by a MSA model?
	\item Can we design systems to analyze different musical features of a musical work in order to understand the individual effects and contributions of each musical dimension? 
	\item Can these sub-analysis on individual musical dimensions help explainability of the overall structure produced by the models, or help adapt their utilities to specific situations?
	\item When analyzing either individual musical features or an aggregate, can considering topological relationships on both the frame level and the section level be used to help specify more nuanced parallelism in different musical aspects?
	\item How can we establish or discover musically meaningful topologies, both on the frame level and the section level, that will aid analysis and improve explainability of model analysis output?
\end{itemize}
